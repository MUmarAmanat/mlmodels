{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copy of run_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaYYlkXFwKRr",
        "colab_type": "code",
        "outputId": "426b3252-5ecd-40d2-b457-628a417f39e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 21.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 81kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 92kB 6.2MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 102kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 112kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 122kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 133kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 143kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 153kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 163kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 174kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 184kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 194kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.17.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (45.1.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVwuCaCMwmrS",
        "colab_type": "code",
        "outputId": "e56e2def-509c-4819-bbab-4a22fd4e020c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "!pip install -q pytorch_transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▉                              | 10kB 35.4MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 174kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 4.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 18.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 870kB 28.8MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ6ncO4_w8XZ",
        "colab_type": "code",
        "outputId": "d568b74e-fb39-4b06-f1de-0c4013a0446f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "!git clone https://github.com/ThilinaRajapakse/pytorch-transformers-classification.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-transformers-classification'...\n",
            "remote: Enumerating objects: 147, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/147)\u001b[K\rremote: Counting objects:   1% (2/147)\u001b[K\rremote: Counting objects:   2% (3/147)\u001b[K\rremote: Counting objects:   3% (5/147)\u001b[K\rremote: Counting objects:   4% (6/147)\u001b[K\rremote: Counting objects:   5% (8/147)\u001b[K\rremote: Counting objects:   6% (9/147)\u001b[K\rremote: Counting objects:   7% (11/147)\u001b[K\rremote: Counting objects:   8% (12/147)\u001b[K\rremote: Counting objects:   9% (14/147)\u001b[K\rremote: Counting objects:  10% (15/147)\u001b[K\rremote: Counting objects:  11% (17/147)\u001b[K\rremote: Counting objects:  12% (18/147)\u001b[K\rremote: Counting objects:  13% (20/147)\u001b[K\rremote: Counting objects:  14% (21/147)\u001b[K\rremote: Counting objects:  15% (23/147)\u001b[K\rremote: Counting objects:  16% (24/147)\u001b[K\rremote: Counting objects:  17% (25/147)\u001b[K\rremote: Counting objects:  18% (27/147)\u001b[K\rremote: Counting objects:  19% (28/147)\u001b[K\rremote: Counting objects:  20% (30/147)\u001b[K\rremote: Counting objects:  21% (31/147)\u001b[K\rremote: Counting objects:  22% (33/147)\u001b[K\rremote: Counting objects:  23% (34/147)\u001b[K\rremote: Counting objects:  24% (36/147)\u001b[K\rremote: Counting objects:  25% (37/147)\u001b[K\rremote: Counting objects:  26% (39/147)\u001b[K\rremote: Counting objects:  27% (40/147)\u001b[K\rremote: Counting objects:  28% (42/147)\u001b[K\rremote: Counting objects:  29% (43/147)\u001b[K\rremote: Counting objects:  30% (45/147)\u001b[K\rremote: Counting objects:  31% (46/147)\u001b[K\rremote: Counting objects:  32% (48/147)\u001b[K\rremote: Counting objects:  33% (49/147)\u001b[K\rremote: Counting objects:  34% (50/147)\u001b[K\rremote: Counting objects:  35% (52/147)\u001b[K\rremote: Counting objects:  36% (53/147)\u001b[K\rremote: Counting objects:  37% (55/147)\u001b[K\rremote: Counting objects:  38% (56/147)\u001b[K\rremote: Counting objects:  39% (58/147)\u001b[K\rremote: Counting objects:  40% (59/147)\u001b[K\rremote: Counting objects:  41% (61/147)\u001b[K\rremote: Counting objects:  42% (62/147)\u001b[K\rremote: Counting objects:  43% (64/147)\u001b[K\rremote: Counting objects:  44% (65/147)\u001b[K\rremote: Counting objects:  45% (67/147)\u001b[K\rremote: Counting objects:  46% (68/147)\u001b[K\rremote: Counting objects:  47% (70/147)\u001b[K\rremote: Counting objects:  48% (71/147)\u001b[K\rremote: Counting objects:  49% (73/147)\u001b[K\rremote: Counting objects:  50% (74/147)\u001b[K\rremote: Counting objects:  51% (75/147)\u001b[K\rremote: Counting objects:  52% (77/147)\u001b[K\rremote: Counting objects:  53% (78/147)\u001b[K\rremote: Counting objects:  54% (80/147)\u001b[K\rremote: Counting objects:  55% (81/147)\u001b[K\rremote: Counting objects:  56% (83/147)\u001b[K\rremote: Counting objects:  57% (84/147)\u001b[K\rremote: Counting objects:  58% (86/147)\u001b[K\rremote: Counting objects:  59% (87/147)\u001b[K\rremote: Counting objects:  60% (89/147)\u001b[K\rremote: Counting objects:  61% (90/147)\u001b[K\rremote: Counting objects:  62% (92/147)\u001b[K\rremote: Counting objects:  63% (93/147)\u001b[K\rremote: Counting objects:  64% (95/147)\u001b[K\rremote: Counting objects:  65% (96/147)\u001b[K\rremote: Counting objects:  66% (98/147)\u001b[K\rremote: Counting objects:  67% (99/147)\u001b[K\rremote: Counting objects:  68% (100/147)\u001b[K\rremote: Counting objects:  69% (102/147)\u001b[K\rremote: Counting objects:  70% (103/147)\u001b[K\rremote: Counting objects:  71% (105/147)\u001b[K\rremote: Counting objects:  72% (106/147)\u001b[K\rremote: Counting objects:  73% (108/147)\u001b[K\rremote: Counting objects:  74% (109/147)\u001b[K\rremote: Counting objects:  75% (111/147)\u001b[K\rremote: Counting objects:  76% (112/147)\u001b[K\rremote: Counting objects:  77% (114/147)\u001b[K\rremote: Counting objects:  78% (115/147)\u001b[K\rremote: Counting objects:  79% (117/147)\u001b[K\rremote: Counting objects:  80% (118/147)\u001b[K\rremote: Counting objects:  81% (120/147)\u001b[K\rremote: Counting objects:  82% (121/147)\u001b[K\rremote: Counting objects:  83% (123/147)\u001b[K\rremote: Counting objects:  84% (124/147)\u001b[K\rremote: Counting objects:  85% (125/147)\u001b[K\rremote: Counting objects:  86% (127/147)\u001b[K\rremote: Counting objects:  87% (128/147)\u001b[K\rremote: Counting objects:  88% (130/147)\u001b[K\rremote: Counting objects:  89% (131/147)\u001b[K\rremote: Counting objects:  90% (133/147)\u001b[K\rremote: Counting objects:  91% (134/147)\u001b[K\rremote: Counting objects:  92% (136/147)\u001b[K\rremote: Counting objects:  93% (137/147)\u001b[K\rremote: Counting objects:  94% (139/147)\u001b[K\rremote: Counting objects:  95% (140/147)\u001b[K\rremote: Counting objects:  96% (142/147)\u001b[K\rremote: Counting objects:  97% (143/147)\u001b[K\rremote: Counting objects:  98% (145/147)\u001b[K\rremote: Counting objects:  99% (146/147)\u001b[K\rremote: Counting objects: 100% (147/147)\u001b[K\rremote: Counting objects: 100% (147/147), done.\u001b[K\n",
            "remote: Compressing objects:   0% (1/129)\u001b[K\rremote: Compressing objects:   1% (2/129)\u001b[K\rremote: Compressing objects:   2% (3/129)\u001b[K\rremote: Compressing objects:   3% (4/129)\u001b[K\rremote: Compressing objects:   4% (6/129)\u001b[K\rremote: Compressing objects:   5% (7/129)\u001b[K\rremote: Compressing objects:   6% (8/129)\u001b[K\rremote: Compressing objects:   7% (10/129)\u001b[K\rremote: Compressing objects:   8% (11/129)\u001b[K\rremote: Compressing objects:   9% (12/129)\u001b[K\rremote: Compressing objects:  10% (13/129)\u001b[K\rremote: Compressing objects:  11% (15/129)\u001b[K\rremote: Compressing objects:  12% (16/129)\u001b[K\rremote: Compressing objects:  13% (17/129)\u001b[K\rremote: Compressing objects:  14% (19/129)\u001b[K\rremote: Compressing objects:  15% (20/129)\u001b[K\rremote: Compressing objects:  16% (21/129)\u001b[K\rremote: Compressing objects:  17% (22/129)\u001b[K\rremote: Compressing objects:  18% (24/129)\u001b[K\rremote: Compressing objects:  19% (25/129)\u001b[K\rremote: Compressing objects:  20% (26/129)\u001b[K\rremote: Compressing objects:  21% (28/129)\u001b[K\rremote: Compressing objects:  22% (29/129)\u001b[K\rremote: Compressing objects:  23% (30/129)\u001b[K\rremote: Compressing objects:  24% (31/129)\u001b[K\rremote: Compressing objects:  25% (33/129)\u001b[K\rremote: Compressing objects:  26% (34/129)\u001b[K\rremote: Compressing objects:  27% (35/129)\u001b[K\rremote: Compressing objects:  28% (37/129)\u001b[K\rremote: Compressing objects:  29% (38/129)\u001b[K\rremote: Compressing objects:  30% (39/129)\u001b[K\rremote: Compressing objects:  31% (40/129)\u001b[K\rremote: Compressing objects:  32% (42/129)\u001b[K\rremote: Compressing objects:  33% (43/129)\u001b[K\rremote: Compressing objects:  34% (44/129)\u001b[K\rremote: Compressing objects:  35% (46/129)\u001b[K\rremote: Compressing objects:  36% (47/129)\u001b[K\rremote: Compressing objects:  37% (48/129)\u001b[K\rremote: Compressing objects:  38% (50/129)\u001b[K\rremote: Compressing objects:  39% (51/129)\u001b[K\rremote: Compressing objects:  40% (52/129)\u001b[K\rremote: Compressing objects:  41% (53/129)\u001b[K\rremote: Compressing objects:  42% (55/129)\u001b[K\rremote: Compressing objects:  43% (56/129)\u001b[K\rremote: Compressing objects:  44% (57/129)\u001b[K\rremote: Compressing objects:  45% (59/129)\u001b[K\rremote: Compressing objects:  46% (60/129)\u001b[K\rremote: Compressing objects:  47% (61/129)\u001b[K\rremote: Compressing objects:  48% (62/129)\u001b[K\rremote: Compressing objects:  49% (64/129)\u001b[K\rremote: Compressing objects:  50% (65/129)\u001b[K\rremote: Compressing objects:  51% (66/129)\u001b[K\rremote: Compressing objects:  52% (68/129)\u001b[K\rremote: Compressing objects:  53% (69/129)\u001b[K\rremote: Compressing objects:  54% (70/129)\u001b[K\rremote: Compressing objects:  55% (71/129)\u001b[K\rremote: Compressing objects:  56% (73/129)\u001b[K\rremote: Compressing objects:  57% (74/129)\u001b[K\rremote: Compressing objects:  58% (75/129)\u001b[K\rremote: Compressing objects:  59% (77/129)\u001b[K\rremote: Compressing objects:  60% (78/129)\u001b[K\rremote: Compressing objects:  61% (79/129)\u001b[K\rremote: Compressing objects:  62% (80/129)\u001b[K\rremote: Compressing objects:  63% (82/129)\u001b[K\rremote: Compressing objects:  64% (83/129)\u001b[K\rremote: Compressing objects:  65% (84/129)\u001b[K\rremote: Compressing objects:  66% (86/129)\u001b[K\rremote: Compressing objects:  67% (87/129)\u001b[K\rremote: Compressing objects:  68% (88/129)\u001b[K\rremote: Compressing objects:  69% (90/129)\u001b[K\rremote: Compressing objects:  70% (91/129)\u001b[K\rremote: Compressing objects:  71% (92/129)\u001b[K\rremote: Compressing objects:  72% (93/129)\u001b[K\rremote: Compressing objects:  73% (95/129)\u001b[K\rremote: Compressing objects:  74% (96/129)\u001b[K\rremote: Compressing objects:  75% (97/129)\u001b[K\rremote: Compressing objects:  76% (99/129)\u001b[K\rremote: Compressing objects:  77% (100/129)\u001b[K\rremote: Compressing objects:  78% (101/129)\u001b[K\rremote: Compressing objects:  79% (102/129)\u001b[K\rremote: Compressing objects:  80% (104/129)\u001b[K\rremote: Compressing objects:  81% (105/129)\u001b[K\rremote: Compressing objects:  82% (106/129)\u001b[K\rremote: Compressing objects:  83% (108/129)\u001b[K\rremote: Compressing objects:  84% (109/129)\u001b[K\rremote: Compressing objects:  85% (110/129)\u001b[K\rremote: Compressing objects:  86% (111/129)\u001b[K\rremote: Compressing objects:  87% (113/129)\u001b[K\rremote: Compressing objects:  88% (114/129)\u001b[K\rremote: Compressing objects:  89% (115/129)\u001b[K\rremote: Compressing objects:  90% (117/129)\u001b[K\rremote: Compressing objects:  91% (118/129)\u001b[K\rremote: Compressing objects:  92% (119/129)\u001b[K\rremote: Compressing objects:  93% (120/129)\u001b[K\rremote: Compressing objects:  94% (122/129)\u001b[K\rremote: Compressing objects:  95% (123/129)\u001b[K\rremote: Compressing objects:  96% (124/129)\u001b[K\rremote: Compressing objects:  97% (126/129)\u001b[K\rremote: Compressing objects:  98% (127/129)\u001b[K\rremote: Compressing objects:  99% (128/129)\u001b[K\rremote: Compressing objects: 100% (129/129)\u001b[K\rremote: Compressing objects: 100% (129/129), done.\u001b[K\n",
            "Receiving objects:   0% (1/147)   \rReceiving objects:   1% (2/147)   \rReceiving objects:   2% (3/147)   \rReceiving objects:   3% (5/147)   \rReceiving objects:   4% (6/147)   \rReceiving objects:   5% (8/147)   \rReceiving objects:   6% (9/147)   \rReceiving objects:   7% (11/147)   \rReceiving objects:   8% (12/147)   \rReceiving objects:   9% (14/147)   \rReceiving objects:  10% (15/147)   \rReceiving objects:  11% (17/147)   \rReceiving objects:  12% (18/147)   \rReceiving objects:  13% (20/147)   \rReceiving objects:  14% (21/147)   \rReceiving objects:  15% (23/147)   \rReceiving objects:  16% (24/147)   \rReceiving objects:  17% (25/147)   \rReceiving objects:  18% (27/147)   \rReceiving objects:  19% (28/147)   \rReceiving objects:  20% (30/147)   \rReceiving objects:  21% (31/147)   \rReceiving objects:  22% (33/147)   \rReceiving objects:  23% (34/147)   \rReceiving objects:  24% (36/147)   \rReceiving objects:  25% (37/147)   \rReceiving objects:  26% (39/147)   \rReceiving objects:  27% (40/147)   \rReceiving objects:  28% (42/147)   \rReceiving objects:  29% (43/147)   \rReceiving objects:  30% (45/147)   \rReceiving objects:  31% (46/147)   \rReceiving objects:  32% (48/147)   \rReceiving objects:  33% (49/147)   \rReceiving objects:  34% (50/147)   \rReceiving objects:  35% (52/147)   \rReceiving objects:  36% (53/147)   \rReceiving objects:  37% (55/147)   \rReceiving objects:  38% (56/147)   \rReceiving objects:  39% (58/147)   \rReceiving objects:  40% (59/147)   \rReceiving objects:  41% (61/147)   \rReceiving objects:  42% (62/147)   \rremote: Total 147 (delta 89), reused 40 (delta 18), pack-reused 0\u001b[K\n",
            "Receiving objects:  43% (64/147)   \rReceiving objects:  44% (65/147)   \rReceiving objects:  45% (67/147)   \rReceiving objects:  46% (68/147)   \rReceiving objects:  47% (70/147)   \rReceiving objects:  48% (71/147)   \rReceiving objects:  49% (73/147)   \rReceiving objects:  50% (74/147)   \rReceiving objects:  51% (75/147)   \rReceiving objects:  52% (77/147)   \rReceiving objects:  53% (78/147)   \rReceiving objects:  54% (80/147)   \rReceiving objects:  55% (81/147)   \rReceiving objects:  56% (83/147)   \rReceiving objects:  57% (84/147)   \rReceiving objects:  58% (86/147)   \rReceiving objects:  59% (87/147)   \rReceiving objects:  60% (89/147)   \rReceiving objects:  61% (90/147)   \rReceiving objects:  62% (92/147)   \rReceiving objects:  63% (93/147)   \rReceiving objects:  64% (95/147)   \rReceiving objects:  65% (96/147)   \rReceiving objects:  66% (98/147)   \rReceiving objects:  67% (99/147)   \rReceiving objects:  68% (100/147)   \rReceiving objects:  69% (102/147)   \rReceiving objects:  70% (103/147)   \rReceiving objects:  71% (105/147)   \rReceiving objects:  72% (106/147)   \rReceiving objects:  73% (108/147)   \rReceiving objects:  74% (109/147)   \rReceiving objects:  75% (111/147)   \rReceiving objects:  76% (112/147)   \rReceiving objects:  77% (114/147)   \rReceiving objects:  78% (115/147)   \rReceiving objects:  79% (117/147)   \rReceiving objects:  80% (118/147)   \rReceiving objects:  81% (120/147)   \rReceiving objects:  82% (121/147)   \rReceiving objects:  83% (123/147)   \rReceiving objects:  84% (124/147)   \rReceiving objects:  85% (125/147)   \rReceiving objects:  86% (127/147)   \rReceiving objects:  87% (128/147)   \rReceiving objects:  88% (130/147)   \rReceiving objects:  89% (131/147)   \rReceiving objects:  90% (133/147)   \rReceiving objects:  91% (134/147)   \rReceiving objects:  92% (136/147)   \rReceiving objects:  93% (137/147)   \rReceiving objects:  94% (139/147)   \rReceiving objects:  95% (140/147)   \rReceiving objects:  96% (142/147)   \rReceiving objects:  97% (143/147)   \rReceiving objects:  98% (145/147)   \rReceiving objects:  99% (146/147)   \rReceiving objects: 100% (147/147)   \rReceiving objects: 100% (147/147), 53.00 KiB | 7.57 MiB/s, done.\n",
            "Resolving deltas:   0% (0/89)   \rResolving deltas:  29% (26/89)   \rResolving deltas:  34% (31/89)   \rResolving deltas:  55% (49/89)   \rResolving deltas:  73% (65/89)   \rResolving deltas:  87% (78/89)   \rResolving deltas:  91% (81/89)   \rResolving deltas:  93% (83/89)   \rResolving deltas: 100% (89/89)   \rResolving deltas: 100% (89/89), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCwYo7OYw985",
        "colab_type": "code",
        "outputId": "8ab4e332-0a22-40c7-a05b-963fc59a637a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd pytorch-transformers-classification"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/pytorch-transformers-classification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sScFVTU_xjzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPkuD2k6xB3H",
        "colab_type": "code",
        "outputId": "a19adb46-4121-4a95-b0fc-64036854242d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "!wget https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz -O ./data/data.tgz\n",
        "!tar -xvzf data/data.tgz -C ./data/\n",
        "!mv ./data/yelp_review_polarity_csv/* ./data/\n",
        "!rm -r ./data/yelp_review_polarity_csv/\n",
        "!rm ./data/data.tgz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-16 07:45:27--  https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.200.149\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.200.149|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 166373201 (159M) [application/x-tar]\n",
            "Saving to: ‘./data/data.tgz’\n",
            "\n",
            "./data/data.tgz     100%[===================>] 158.67M  42.0MB/s    in 4.0s    \n",
            "\n",
            "2020-02-16 07:45:32 (39.9 MB/s) - ‘./data/data.tgz’ saved [166373201/166373201]\n",
            "\n",
            "yelp_review_polarity_csv/\n",
            "yelp_review_polarity_csv/train.csv\n",
            "yelp_review_polarity_csv/readme.txt\n",
            "yelp_review_polarity_csv/test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHXGIDgRwaw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "import random\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import tqdm_notebook, trange\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "from pytorch_transformers import (WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer,\n",
        "                                  XLMConfig, XLMForSequenceClassification, XLMTokenizer, \n",
        "                                  XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer,\n",
        "                                  RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "\n",
        "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
        "\n",
        "from utils import (convert_examples_to_features,\n",
        "                        output_modes, processors)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynvkW5v0yaI8",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZNHvzO6yCn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "prefix = './data/'\n",
        "\n",
        "train_df = pd.read_csv(prefix + 'train.csv', header=None)\n",
        "train_df.head()\n",
        "\n",
        "test_df = pd.read_csv(prefix + 'test.csv', header=None)\n",
        "test_df.head()\n",
        "\n",
        "train_df[0] = (train_df[0] == 2).astype(int)\n",
        "test_df[0] = (test_df[0] == 2).astype(int)\n",
        "\n",
        "train_df = pd.DataFrame({\n",
        "    'id':range(len(train_df)),\n",
        "    'label':train_df[0],\n",
        "    'alpha':['a']*train_df.shape[0],\n",
        "    'text': train_df[1].replace(r'\\n', ' ', regex=True)\n",
        "})\n",
        "\n",
        "train_df.head()\n",
        "\n",
        "dev_df = pd.DataFrame({\n",
        "    'id':range(len(test_df)),\n",
        "    'label':test_df[0],\n",
        "    'alpha':['a']*test_df.shape[0],\n",
        "    'text': test_df[1].replace(r'\\n', ' ', regex=True)\n",
        "})\n",
        "\n",
        "dev_df.head()\n",
        "\n",
        "\n",
        "train_df.to_csv('./data/train.tsv', sep='\\t', index=False, header=False, columns=train_df.columns)\n",
        "dev_df.to_csv('./data/dev.tsv', sep='\\t', index=False, header=False, columns=dev_df.columns)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rinM5OHTwKRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {\n",
        "    'data_dir': './data/',\n",
        "    'model_type':  'xlnet',\n",
        "    'model_name': 'xlnet-base-cased',\n",
        "    'task_name': 'binary',\n",
        "    'output_dir': './outputs/',\n",
        "    'cache_dir': './cache/',\n",
        "    'do_train': True,\n",
        "    'do_eval': True,\n",
        "    'fp16': True,\n",
        "    'fp16_opt_level': 'O1',\n",
        "    'max_seq_length': 128,\n",
        "    'output_mode': 'classification',\n",
        "    'train_batch_size': 8,\n",
        "    'eval_batch_size': 8,\n",
        "\n",
        "    'gradient_accumulation_steps': 1,\n",
        "    'num_train_epochs': 1,\n",
        "    'weight_decay': 0,\n",
        "    'learning_rate': 4e-5,\n",
        "    'adam_epsilon': 1e-8,\n",
        "    'warmup_ratio': 0.06,\n",
        "    'warmup_steps': 0,\n",
        "    'max_grad_norm': 1.0,\n",
        "\n",
        "    'logging_steps': 50,\n",
        "    'evaluate_during_training': False,\n",
        "    'save_steps': 2000,\n",
        "    'eval_all_checkpoints': True,\n",
        "\n",
        "    'overwrite_output_dir': False,\n",
        "    'reprocess_input_data': True,\n",
        "    'notes': 'Using Yelp Reviews dataset'\n",
        "}\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6YTlOK8wKR2",
        "colab_type": "code",
        "outputId": "fc5c86ba-9ee7-4f46-caa1-ec4353c1b60d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "source": [
        "args"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'adam_epsilon': 1e-08,\n",
              " 'cache_dir': './cache/',\n",
              " 'data_dir': './data/',\n",
              " 'do_eval': True,\n",
              " 'do_train': True,\n",
              " 'eval_all_checkpoints': True,\n",
              " 'eval_batch_size': 8,\n",
              " 'evaluate_during_training': False,\n",
              " 'fp16': True,\n",
              " 'fp16_opt_level': 'O1',\n",
              " 'gradient_accumulation_steps': 1,\n",
              " 'learning_rate': 4e-05,\n",
              " 'logging_steps': 50,\n",
              " 'max_grad_norm': 1.0,\n",
              " 'max_seq_length': 128,\n",
              " 'model_name': 'xlnet-base-cased',\n",
              " 'model_type': 'xlnet',\n",
              " 'notes': 'Using Yelp Reviews dataset',\n",
              " 'num_train_epochs': 1,\n",
              " 'output_dir': './outputs/',\n",
              " 'output_mode': 'classification',\n",
              " 'overwrite_output_dir': False,\n",
              " 'reprocess_input_data': True,\n",
              " 'save_steps': 2000,\n",
              " 'task_name': 'binary',\n",
              " 'train_batch_size': 8,\n",
              " 'warmup_ratio': 0.06,\n",
              " 'warmup_steps': 0,\n",
              " 'weight_decay': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmyZWna5wKR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('args.json', 'w') as f:\n",
        "    json.dump(args, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixdVojLWwKR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if os.path.exists(args['output_dir']) and os.listdir(args['output_dir']) and args['do_train'] and not args['overwrite_output_dir']:\n",
        "    raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args['output_dir']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGA2aqr0wKSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_CLASSES = {\n",
        "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
        "    'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
        "    'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
        "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "}\n",
        "\n",
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[args['model_type']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhRm4kf2wKSG",
        "colab_type": "code",
        "outputId": "4d0d09f7-c203-4a5d-dae2-22cb75ca56d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        }
      },
      "source": [
        "config = config_class.from_pretrained(args['model_name'], num_labels=2, finetuning_task=args['task_name'])\n",
        "tokenizer = tokenizer_class.from_pretrained(args['model_name'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at /root/.cache/torch/pytorch_transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.8df552e150a401a37ae808caf2a2c86fb6fedaa1f6963d1f21fbf3d0085c9e74\n",
            "INFO:pytorch_transformers.modeling_utils:Model config {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": \"binary\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_token\": 32000,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pruned_heads\": {},\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"torchscript\": false,\n",
            "  \"untie_r\": true\n",
            "}\n",
            "\n",
            "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model from cache at /root/.cache/torch/pytorch_transformers/dad589d582573df0293448af5109cb6981ca77239ed314e15ca63b7b8a318ddd.8b10bd978b5d01c21303cc761fc9ecd464419b3bf921864a355ba807cfbfafa8\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5_E7JbTwKSJ",
        "colab_type": "code",
        "outputId": "d5246437-daea-4da0-fa75-7f50c3f634c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        }
      },
      "source": [
        "model = model_class.from_pretrained(args['model_name'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at /root/.cache/torch/pytorch_transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.8df552e150a401a37ae808caf2a2c86fb6fedaa1f6963d1f21fbf3d0085c9e74\n",
            "INFO:pytorch_transformers.modeling_utils:Model config {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": null,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_token\": 32000,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pruned_heads\": {},\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"torchscript\": false,\n",
            "  \"untie_r\": true\n",
            "}\n",
            "\n",
            "INFO:pytorch_transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/24197ba0ce5dbfe23924431610704c88e2c0371afa49149360e4c823219ab474.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac\n",
            "INFO:pytorch_transformers.modeling_utils:Weights of XLNetForSequenceClassification not initialized from pretrained model: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
            "INFO:pytorch_transformers.modeling_utils:Weights from pretrained model not used in XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cjGt5WiwKSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.to(device);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD9VjnK1wKSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "task = args['task_name']\n",
        "\n",
        "if task in processors.keys() and task in output_modes.keys():\n",
        "    processor = processors[task]()\n",
        "    label_list = processor.get_labels()\n",
        "    num_labels = len(label_list)\n",
        "else:\n",
        "    raise KeyError(f'{task} not found in processors or in output_modes. Please check utils.py.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SzQYAMwwKSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_and_cache_examples(task, tokenizer, evaluate=False):\n",
        "    processor = processors[task]()\n",
        "    output_mode = args['output_mode']\n",
        "    \n",
        "    mode = 'dev' if evaluate else 'train'\n",
        "    cached_features_file = os.path.join(args['data_dir'], f\"cached_{mode}_{args['model_name']}_{args['max_seq_length']}_{task}\")\n",
        "    \n",
        "    if os.path.exists(cached_features_file) and not args['reprocess_input_data']:\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "               \n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", args['data_dir'])\n",
        "        label_list = processor.get_labels()\n",
        "        examples = processor.get_dev_examples(args['data_dir']) if evaluate else processor.get_train_examples(args['data_dir'])\n",
        "        \n",
        "        if __name__ == \"__main__\":\n",
        "            features = convert_examples_to_features(examples, label_list, args['max_seq_length'], tokenizer, output_mode,\n",
        "                cls_token_at_end=bool(args['model_type'] in ['xlnet']),            # xlnet has a cls token at the end\n",
        "                cls_token=tokenizer.cls_token,\n",
        "                cls_token_segment_id=2 if args['model_type'] in ['xlnet'] else 0,\n",
        "                sep_token=tokenizer.sep_token,\n",
        "                sep_token_extra=bool(args['model_type'] in ['roberta']),           # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n",
        "                pad_on_left=bool(args['model_type'] in ['xlnet']),                 # pad on the left for xlnet\n",
        "                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
        "                pad_token_segment_id=4 if args['model_type'] in ['xlnet'] else 0)\n",
        "        \n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        torch.save(features, cached_features_file)\n",
        "        \n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "    if output_mode == \"classification\":\n",
        "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
        "    elif output_mode == \"regression\":\n",
        "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZzxOKyAwKSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_dataset, model, tokenizer):\n",
        "    tb_writer = SummaryWriter()\n",
        "    \n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args['train_batch_size'])\n",
        "    \n",
        "    t_total = len(train_dataloader) // args['gradient_accumulation_steps'] * args['num_train_epochs']\n",
        "    \n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args['weight_decay']},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "    \n",
        "    warmup_steps = math.ceil(t_total * args['warmup_ratio'])\n",
        "    args['warmup_steps'] = warmup_steps if args['warmup_steps'] == 0 else args['warmup_steps']\n",
        "    \n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=args['adam_epsilon'])\n",
        "    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args['warmup_steps'], t_total=t_total)\n",
        "    \n",
        "    if args['fp16']:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args['fp16_opt_level'])\n",
        "        \n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n",
        "    logger.info(\"  Total train batch size  = %d\", args['train_batch_size'])\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args['gradient_accumulation_steps'])\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\")\n",
        "    \n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm_notebook(train_dataloader, desc=\"Iteration\")\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            model.train()\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            inputs = {'input_ids':      batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
        "                      'labels':         batch[3]}\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
        "            print(\"\\r%f\" % loss, end='')\n",
        "\n",
        "            if args['gradient_accumulation_steps'] > 1:\n",
        "                loss = loss / args['gradient_accumulation_steps']\n",
        "\n",
        "            if args['fp16']:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args['max_grad_norm'])\n",
        "                \n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args['logging_steps'] > 0 and global_step % args['logging_steps'] == 0:\n",
        "                    # Log metrics\n",
        "                    if args['evaluate_during_training']:  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results, _ = evaluate(model, tokenizer)\n",
        "                        for key, value in results.items():\n",
        "                            tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
        "                    tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
        "                    tb_writer.add_scalar('loss', (tr_loss - logging_loss)/args['logging_steps'], global_step)\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                if args['save_steps'] > 0 and global_step % args['save_steps'] == 0:\n",
        "                    # Save model checkpoint\n",
        "                    output_dir = os.path.join(args['output_dir'], 'checkpoint-{}'.format(global_step))\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "\n",
        "    return global_step, tr_loss / global_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B020wxLWwKSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error, matthews_corrcoef, confusion_matrix\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "def get_mismatched(labels, preds):\n",
        "    mismatched = labels != preds\n",
        "    examples = processor.get_dev_examples(args['data_dir'])\n",
        "    wrong = [i for (i, v) in zip(examples, mismatched) if v]\n",
        "    \n",
        "    return wrong\n",
        "\n",
        "def get_eval_report(labels, preds):\n",
        "    mcc = matthews_corrcoef(labels, preds)\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
        "    return {\n",
        "        \"mcc\": mcc,\n",
        "        \"tp\": tp,\n",
        "        \"tn\": tn,\n",
        "        \"fp\": fp,\n",
        "        \"fn\": fn\n",
        "    }, get_mismatched(labels, preds)\n",
        "\n",
        "def compute_metrics(task_name, preds, labels):\n",
        "    assert len(preds) == len(labels)\n",
        "    return get_eval_report(labels, preds)\n",
        "\n",
        "def evaluate(model, tokenizer, prefix=\"\"):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_output_dir = args['output_dir']\n",
        "\n",
        "    results = {}\n",
        "    EVAL_TASK = args['task_name']\n",
        "\n",
        "    eval_dataset = load_and_cache_examples(EVAL_TASK, tokenizer, evaluate=True)\n",
        "    if not os.path.exists(eval_output_dir):\n",
        "        os.makedirs(eval_output_dir)\n",
        "\n",
        "\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args['eval_batch_size'])\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    preds = None\n",
        "    out_label_ids = None\n",
        "    for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {'input_ids':      batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
        "                      'labels':         batch[3]}\n",
        "            outputs = model(**inputs)\n",
        "            tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "        if preds is None:\n",
        "            preds = logits.detach().cpu().numpy()\n",
        "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
        "        else:\n",
        "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    if args['output_mode'] == \"classification\":\n",
        "        preds = np.argmax(preds, axis=1)\n",
        "    elif args['output_mode'] == \"regression\":\n",
        "        preds = np.squeeze(preds)\n",
        "    result, wrong = compute_metrics(EVAL_TASK, preds, out_label_ids)\n",
        "    results.update(result)\n",
        "\n",
        "    output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
        "    with open(output_eval_file, \"w\") as writer:\n",
        "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
        "        for key in sorted(result.keys()):\n",
        "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "    return results, wrong"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXUXu8WM6qHC",
        "colab_type": "code",
        "outputId": "d9ce1bae-0d41-45e8-dc53-bda6e14136e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "args.json\t\tdata_prep.ipynb  run_model.ipynb\n",
            "colab_quickstart.ipynb\tLICENSE\t\t transformer_classifier.py\n",
            "data\t\t\t__pycache__\t utils.py\n",
            "data_download.sh\tREADME.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO-80Kjh6klt",
        "colab_type": "code",
        "outputId": "2847446f-0f7a-45d4-b663-79ac7aad94ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python ./transformer_classifier.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ('#### Loading params   ##############################################',) \n",
            "\n",
            "  ('#### Model init, fit   #############################################',) \n",
            "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json not found in cache or force_download set to True, downloading to /tmp/tmp5kb2i7_e\n",
            "100% 690/690 [00:00<00:00, 676975.38B/s]\n",
            "INFO:pytorch_transformers.file_utils:copying /tmp/tmp5kb2i7_e to cache at /root/.cache/torch/pytorch_transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.8df552e150a401a37ae808caf2a2c86fb6fedaa1f6963d1f21fbf3d0085c9e74\n",
            "INFO:pytorch_transformers.file_utils:creating metadata file for /root/.cache/torch/pytorch_transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.8df552e150a401a37ae808caf2a2c86fb6fedaa1f6963d1f21fbf3d0085c9e74\n",
            "INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmp5kb2i7_e\n",
            "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at /root/.cache/torch/pytorch_transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.8df552e150a401a37ae808caf2a2c86fb6fedaa1f6963d1f21fbf3d0085c9e74\n",
            "INFO:pytorch_transformers.modeling_utils:Model config {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": \"binary\",\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_token\": 32000,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pruned_heads\": {},\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"torchscript\": false,\n",
            "  \"untie_r\": true\n",
            "}\n",
            "\n",
            "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model not found in cache or force_download set to True, downloading to /tmp/tmp2lo5icnd\n",
            "100% 798011/798011 [00:00<00:00, 2412371.79B/s]\n",
            "INFO:pytorch_transformers.file_utils:copying /tmp/tmp2lo5icnd to cache at /root/.cache/torch/pytorch_transformers/dad589d582573df0293448af5109cb6981ca77239ed314e15ca63b7b8a318ddd.8b10bd978b5d01c21303cc761fc9ecd464419b3bf921864a355ba807cfbfafa8\n",
            "INFO:pytorch_transformers.file_utils:creating metadata file for /root/.cache/torch/pytorch_transformers/dad589d582573df0293448af5109cb6981ca77239ed314e15ca63b7b8a318ddd.8b10bd978b5d01c21303cc761fc9ecd464419b3bf921864a355ba807cfbfafa8\n",
            "INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmp2lo5icnd\n",
            "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model from cache at /root/.cache/torch/pytorch_transformers/dad589d582573df0293448af5109cb6981ca77239ed314e15ca63b7b8a318ddd.8b10bd978b5d01c21303cc761fc9ecd464419b3bf921864a355ba807cfbfafa8\n",
            "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at /root/.cache/torch/pytorch_transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.8df552e150a401a37ae808caf2a2c86fb6fedaa1f6963d1f21fbf3d0085c9e74\n",
            "INFO:pytorch_transformers.modeling_utils:Model config {\n",
            "  \"architectures\": [\n",
            "    \"XLNetLMHeadModel\"\n",
            "  ],\n",
            "  \"attn_type\": \"bi\",\n",
            "  \"bi_data\": false,\n",
            "  \"clamp_len\": -1,\n",
            "  \"d_head\": 64,\n",
            "  \"d_inner\": 3072,\n",
            "  \"d_model\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"end_n_top\": 5,\n",
            "  \"ff_activation\": \"gelu\",\n",
            "  \"finetuning_task\": null,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"mem_len\": null,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_token\": 32000,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pruned_heads\": {},\n",
            "  \"reuse_len\": null,\n",
            "  \"same_length\": false,\n",
            "  \"start_n_top\": 5,\n",
            "  \"summary_activation\": \"tanh\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"last\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"torchscript\": false,\n",
            "  \"untie_r\": true\n",
            "}\n",
            "\n",
            "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmp0t4auuor\n",
            "100% 467042463/467042463 [00:32<00:00, 14590753.37B/s]\n",
            "INFO:pytorch_transformers.file_utils:copying /tmp/tmp0t4auuor to cache at /root/.cache/torch/pytorch_transformers/24197ba0ce5dbfe23924431610704c88e2c0371afa49149360e4c823219ab474.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac\n",
            "INFO:pytorch_transformers.file_utils:creating metadata file for /root/.cache/torch/pytorch_transformers/24197ba0ce5dbfe23924431610704c88e2c0371afa49149360e4c823219ab474.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac\n",
            "INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmp0t4auuor\n",
            "INFO:pytorch_transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/24197ba0ce5dbfe23924431610704c88e2c0371afa49149360e4c823219ab474.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac\n",
            "INFO:pytorch_transformers.modeling_utils:Weights of XLNetForSequenceClassification not initialized from pretrained model: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
            "INFO:pytorch_transformers.modeling_utils:Weights from pretrained model not used in XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "\n",
            "  ('#### Loading dataset   #############################################',) \n",
            "INFO:__main__:Creating features from dataset file at ./data/\n",
            "Traceback (most recent call last):\n",
            "  File \"./transformer_classifier.py\", line 464, in <module>\n",
            "    test(pars_choice=0)\n",
            "  File \"./transformer_classifier.py\", line 419, in test\n",
            "    train_dataset = get_dataset(task, model.tokenizer)\n",
            "  File \"./transformer_classifier.py\", line 122, in get_dataset\n",
            "    pad_token_segment_id=4 if args['model_type'] in ['xlnet'] else 0)\n",
            "  File \"/content/pytorch-transformers-classification/utils.py\", line 230, in convert_examples_to_features\n",
            "    with Pool(process_count) as p:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/context.py\", line 119, in Pool\n",
            "    context=self.get_context())\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 167, in __init__\n",
            "    raise ValueError(\"Number of processes must be at least 1\")\n",
            "ValueError: Number of processes must be at least 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK_8GIeVwKSd",
        "colab_type": "code",
        "outputId": "a6c0d462-ad38-4a02-95ce-f5427667d9fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        }
      },
      "source": [
        "if args['do_train']:\n",
        "    train_dataset = load_and_cache_examples(task, tokenizer)\n",
        "    global_step, tr_loss = train(train_dataset, model, tokenizer)\n",
        "    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Creating features from dataset file at ./data/\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-03ea62f17108>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'do_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_cache_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" global_step = %s, average loss = %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-264269d57a33>\u001b[0m in \u001b[0;36mload_and_cache_examples\u001b[0;34m(task, tokenizer, evaluate)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mpad_on_left\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'xlnet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0;31m# pad on the left for xlnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mpad_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 pad_token_segment_id=4 if args['model_type'] in ['xlnet'] else 0)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving features into cached file %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_features_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/pytorch-transformers-classification/utils.py\u001b[0m in \u001b[0;36mconvert_examples_to_features\u001b[0;34m(examples, label_list, max_seq_length, tokenizer, output_mode, cls_token_at_end, sep_token_extra, pad_on_left, cls_token, sep_token, pad_token, sequence_a_segment_id, sequence_b_segment_id, cls_token_segment_id, pad_token_segment_id, mask_padding_with_zero, process_count)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_token_at_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_token_segment_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_on_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token_segment_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep_token_extra\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_count\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_example_to_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36mPool\u001b[0;34m(self, processes, initializer, initargs, maxtasksperchild)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         return Pool(processes, initializer, initargs, maxtasksperchild,\n\u001b[0;32m--> 119\u001b[0;31m                     context=self.get_context())\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mRawValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypecode_or_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, processes, initializer, initargs, maxtasksperchild, context)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mprocesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprocesses\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of processes must be at least 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitializer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Number of processes must be at least 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3Wj6VIrwKSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if args['do_train']:\n",
        "    if not os.path.exists(args['output_dir']):\n",
        "            os.makedirs(args['output_dir'])\n",
        "    logger.info(\"Saving model checkpoint to %s\", args['output_dir'])\n",
        "    \n",
        "    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "    model_to_save.save_pretrained(args['output_dir'])\n",
        "    tokenizer.save_pretrained(args['output_dir'])\n",
        "    torch.save(args, os.path.join(args['output_dir'], 'training_args.bin'))    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9ygPfhJwKSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = {}\n",
        "if args['do_eval']:\n",
        "    checkpoints = [args['output_dir']]\n",
        "    if args['eval_all_checkpoints']:\n",
        "        checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args['output_dir'] + '/**/' + WEIGHTS_NAME, recursive=True)))\n",
        "        logging.getLogger(\"pytorch_transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "    logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "    for checkpoint in checkpoints:\n",
        "        global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
        "        model = model_class.from_pretrained(checkpoint)\n",
        "        model.to(device)\n",
        "        result, wrong_preds = evaluate(model, tokenizer, prefix=global_step)\n",
        "        result = dict((k + '_{}'.format(global_step), v) for k, v in result.items())\n",
        "        results.update(result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "738FSW8WwKSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}